"""This module contains utility functions for performing inference using pre-trained sequence classification models."""

# Module: inference
# Author: Tanishq-ids

import argparse
import os
import numpy
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer


def check_model_and_tokenizer_path(model_path, tokenizer_path):
    """
    Check if the model and tokenizer paths are valid.
     
    Args:
        model_path (str): Path to the model file.
        tokenizer_path (str): Path to the tokenizer file.
        
    Raises:
        ValueError: If the model or tokenizer path does not exist.
    """
    if not os.path.exists(model_path):
        raise ValueError(f"Model path {model_path} does not exist.")

    if not os.path.exists(tokenizer_path):
        raise ValueError(f"Tokenizer path {tokenizer_path} does not exist.")


def get_inference(question: str, paragraph: str, model_path: str, tokenizer_path: str) -> int:
    """
    Perform inference using a pre-trained sequence classification model.

    Parameters:
        question (str): The question for inference.
        paragraph (str): The paragraph to be analyzed.
        model_path (str): Path to the pre-trained model.
        tokenizer_path (str): Path to the tokenizer to the pre-trained model.

    Returns:
        int: Predicted label ID.
    """
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    # Tokenize inputs
    inputs = tokenizer(question, paragraph, return_tensors="pt", truncation=True, padding=True, max_length=512)

    # Forward pass
    with torch.no_grad():
        outputs = model(**inputs)

    # Get predicted label
    predicted_label_id = torch.argmax(outputs.logits, dim=1).item()

    return predicted_label_id


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Perform inference using a pre-trained sequence classification model. (local or hugging face)")
    parser.add_argument("--question", type=str, required=True, help="The question for inference.")
    parser.add_argument("--paragraph", type=str, required=True, help="The paragraph to be analyzed.")
    parser.add_argument("--model_path", type=str, required=True, help="Path to the pre-trained model. (local or hugging face)")
    parser.add_argument("--tokenizer_path", type=str, required=True, help="Path to the tokenizer to the pre-trained model. (local or hugging face)")

    args = parser.parse_args()

    check_model_and_tokenizer_path(args.model_path, args.tokenizer_path)

    result = get_inference(
        question=args.question,
        paragraph=args.paragraph,
        model_path=args.model_path,
        tokenizer_path=args.tokenizer_path
    )

    print(f"Predicted Label ID: {result}")



'''python inference.py 
    --question "What is the capital of France?" 
    --paragraph "Paris is the capital of France." 
    --model_path /path/to/model    
    --tokenizer_path /path/to/tokenizer'''
